http://gentoo-overlays.zugaina.org/loongson/portage/net-libs/xulrunner/files/xulrunner-chromium-mips.patch

diff --git a/ipc/chromium/src/base/atomicops.h b/ipc/chromium/src/base/atomicops.h
index 87df918..363bf63 100644
--- a/ipc/chromium/src/base/atomicops.h
+++ b/ipc/chromium/src/base/atomicops.h
@@ -132,6 +132,8 @@ Atomic64 Release_Load(volatile const Atomic64* ptr);
 #include "base/atomicops_internals_x86_gcc.h"
 #elif defined(COMPILER_GCC) && defined(ARCH_CPU_ARM_FAMILY)
 #include "base/atomicops_internals_arm_gcc.h"
+#elif defined(COMPILER_GCC) && defined(ARCH_CPU_MIPS_FAMILY)
+#include "base/atomicops_internals_mips_gcc.h"
 #else
 #error "Atomic operations are not supported on your platform"
 #endif
diff --git a/ipc/chromium/src/base/atomicops_internals_mips_gcc.h b/ipc/chromium/src/base/atomicops_internals_mips_gcc.h
new file mode 100644
index 0000000..d1b87ee
--- /dev/null
+++ b/ipc/chromium/src/base/atomicops_internals_mips_gcc.h
@@ -0,0 +1,160 @@
+// Copyright (c) 2010 Zhang, Le <r0bertz@gentoo.org>
+// Use of this source code is governed by GPLv2.
+
+// This file is an internal atomic implementation, use base/atomicops.h instead.
+
+#ifndef BASE_ATOMICOPS_INTERNALS_MIPS_GCC_H_
+#define BASE_ATOMICOPS_INTERNALS_MIPS_GCC_H_
+
+#define ATOMICOPS_COMPILER_BARRIER() __asm__ __volatile__("" : : : "memory")
+
+namespace base {
+namespace subtle {
+
+// 32-bit low-level operations on any platform.
+
+inline Atomic32 NoBarrier_CompareAndSwap(volatile Atomic32* ptr,
+                                         Atomic32 old_value,
+                                         Atomic32 new_value) {
+  Atomic32 prev;
+   __asm__ __volatile__(
+                       " .set  push        \n"
+                       " .set  noat        \n"
+                       " .set  mips3       \n"
+                       "1: ll  %0, %2      \n"
+                       " bne %0, %z3, 2f   \n"
+                       " .set  mips0       \n"
+                       " move  $1, %z4     \n"
+                       " .set  mips3       \n"
+                       " sc  $1, %1        \n"
+                       " beqz  $1, 3f      \n"
+                       "2:                 \n"
+                       " .subsection 2     \n"
+                       "3: b 1b            \n"
+                       " .previous         \n"
+                       " .set  pop         \n"
+                       : "=&r" (prev), "=R" (*ptr)       
+                       : "R" (*ptr), "Jr" (old_value), "Jr" (new_value)     
+                       : "memory");
+  return prev;
+}
+
+inline Atomic32 NoBarrier_AtomicExchange(volatile Atomic32* ptr,
+                                         Atomic32 new_value) {
+  unsigned int ret_value;
+  unsigned long dummy;
+
+  __asm__ __volatile__("       .set    mips3                                   \n"
+                       "1:     ll      %0, %3                  # xchg_u32      \n"
+                       "       .set    mips0                                   \n"
+                       "       move    %2, %z4                                 \n"
+                       "       .set    mips3                                   \n"
+                       "       sc      %2, %1                                  \n"
+                       "       beqz    %2, 2f                                  \n"
+                       "       .subsection 2                                   \n"
+                       "2:     b       1b                                      \n"
+                       "       .previous                                       \n"
+                       "       .set    mips0                                   \n"
+                       : "=&r" (ret_value), "=m" (*ptr), "=&r" (dummy)
+                       : "R" (*ptr), "Jr" (new_value)
+                       : "memory");
+
+  return ret_value;  // Now it's the previous value.
+}
+
+inline Atomic32 NoBarrier_AtomicIncrement(volatile Atomic32* ptr,
+                                          Atomic32 increment) {
+  Atomic32 temp, result;
+    __asm__ __volatile__(
+    " .set  mips3         \n"
+    "1: ll  %1, %2    # atomic_add_return \n"
+    " addu  %0, %1, %3        \n"
+    " sc  %0, %2          \n"
+    " beqz  %0, 2f          \n"
+    " addu  %0, %1, %3        \n"
+    " .subsection 2         \n"
+    "2: b 1b          \n"
+    " .previous         \n"
+    " .set  mips0         \n"
+    : "=&r" (result), "=&r" (temp), "=m" (*ptr)
+    : "Ir" (increment), "m" (*ptr)
+    : "memory");
+  return result;
+}
+
+inline Atomic32 Barrier_AtomicIncrement(volatile Atomic32* ptr,
+                                        Atomic32 increment) {
+  Atomic32 temp, result;
+    __asm__ __volatile__(
+    " .set  mips3         \n"
+    "1: ll  %1, %2    # atomic_add_return \n"
+    " addu  %0, %1, %3        \n"
+    " sc  %0, %2          \n"
+    " beqz  %0, 2f          \n"
+    " addu  %0, %1, %3        \n"
+    " .subsection 2         \n"
+    "2: b 1b          \n"
+    " .previous         \n"
+    " .set  mips0         \n"
+    : "=&r" (result), "=&r" (temp), "=m" (*ptr)
+    : "Ir" (increment), "m" (*ptr)
+    : "memory");
+  __asm__ __volatile__("sync" : : : "memory");
+  return result;
+}
+
+inline Atomic32 Acquire_CompareAndSwap(volatile Atomic32* ptr,
+                                       Atomic32 old_value,
+                                       Atomic32 new_value) {
+  Atomic32 x = NoBarrier_CompareAndSwap(ptr, old_value, new_value);
+  __asm__ __volatile__("sync" : : : "memory");
+  return x;
+}
+
+inline Atomic32 Release_CompareAndSwap(volatile Atomic32* ptr,
+                                       Atomic32 old_value,
+                                       Atomic32 new_value) {
+  return NoBarrier_CompareAndSwap(ptr, old_value, new_value);
+}
+
+inline void NoBarrier_Store(volatile Atomic32* ptr, Atomic32 value) {
+  *ptr = value;
+}
+
+inline void MemoryBarrier() {
+  __asm__ __volatile__("sync" : : : "memory");
+}
+
+inline void Acquire_Store(volatile Atomic32* ptr, Atomic32 value) {
+  *ptr = value;
+  __asm__ __volatile__("sync" : : : "memory");
+}
+
+inline void Release_Store(volatile Atomic32* ptr, Atomic32 value) {
+  ATOMICOPS_COMPILER_BARRIER();
+  *ptr = value; // An x86 store acts as a release barrier.
+  // See comments in Atomic64 version of Release_Store(), below.
+}
+
+inline Atomic32 NoBarrier_Load(volatile const Atomic32* ptr) {
+  return *ptr;
+}
+
+inline Atomic32 Acquire_Load(volatile const Atomic32* ptr) {
+  Atomic32 value = *ptr; // An x86 load acts as a acquire barrier.
+  // See comments in Atomic64 version of Release_Store(), below.
+  ATOMICOPS_COMPILER_BARRIER();
+  return value;
+}
+
+inline Atomic32 Release_Load(volatile const Atomic32* ptr) {
+  MemoryBarrier();
+  return *ptr;
+}
+
+} // namespace base::subtle
+} // namespace base
+
+#undef ATOMICOPS_COMPILER_BARRIER
+
+#endif  // BASE_ATOMICOPS_INTERNALS_MIPS_GCC_H_
diff --git a/ipc/chromium/src/base/debug_util_posix.cc b/ipc/chromium/src/base/debug_util_posix.cc
index f7c58b4..50fb41d 100644
--- a/ipc/chromium/src/base/debug_util_posix.cc
+++ b/ipc/chromium/src/base/debug_util_posix.cc
@@ -108,7 +108,7 @@ bool DebugUtil::BeingDebugged() {
 
 // static
 void DebugUtil::BreakDebugger() {
-#if !defined(ARCH_CPU_ARM_FAMILY)
+#if !defined(ARCH_CPU_ARM_FAMILY) && !defined(ARCH_CPU_MIPS_FAMILY)
   asm ("int3");
 #endif
 }
diff --git a/ipc/chromium/src/build/build_config.h b/ipc/chromium/src/build/build_config.h
index 36f83e7..128bbc7 100644
--- a/ipc/chromium/src/build/build_config.h
+++ b/ipc/chromium/src/build/build_config.h
@@ -57,6 +57,8 @@
 #define ARCH_CPU_ARMEL 1
 #define ARCH_CPU_32_BITS 1
 #define WCHAR_T_IS_UNSIGNED 1
+#elif defined(__MIPSEL__)
+#define ARCH_CPU_MIPS_FAMILY 1
 #else
 #error Please add support for your architecture in build/build_config.h
 #endif
